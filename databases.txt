Disk -> Ram(many gigabytes) -> CPU cache (l1 32kb, l2 64kb, l3 8Mb)


Lookup L1 -> L2 -> L3 -> Ram -> Disk

When it finds it in Ram pulls "cache line"  ~ 64 consecutive bytes it pulls
it into the cache and it exists in L3, L2, and L1. Since cores
share L3 cache multi core computations can share this cache line.


Column databases benefit from column values of multiple rows in sequential memory because
cache lines will contain values from many rows so you don't have to return to disk,(Average salary over a table
only need to use one column and all the values exist next to each other)


Pages: Chunks of memory ~ 3kb

Cache Line: Amount of sequential data pulled into cache
Say you have address:

                  1011011010110
and you want the cache line from there,

Cache line key -> 101101_______ 
zero out the last 6 bits after the key, they all need to be in the same page(2 ^ 6 = 64)

Cache Line 64 bits

--Daytomic

Threads - compute abstraction(process of execution abstraction)
Process - Memory abstraction


mmap - suite of protocols for memory sharing between processes
